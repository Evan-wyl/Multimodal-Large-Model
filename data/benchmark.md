## Vision Language Benchmark

| Name                                                         | Features                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [GQA](https://arxiv.org/abs/1902.09506)                      | Real-World Visual Reasoning and Compositional Question Answering. |
| [HM: HatefulMemes](https://arxiv.org/abs/2005.04790)         |                                                              |
| [IconVQA](https://iconqa.github.io/)                         | Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. |
| [LLaVA-W: LLaVA-Bench (In-the-Wild)](https://arxiv.org/abs/2205.00363) |                                                              |
| [MME-P:  MME Perception](https://arxiv.org/abs/2306.13394)   | A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. |
| [MME-C:  MME Cognition](https://arxiv.org/abs/2306.13394)    | A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. |
| [MMB :  MMBenchmark](https://arxiv.org/abs/2307.06281)       | A systematically-designed objective benchmark for robustly evaluating the various abilities of vision-language models. |
| [MMB-CN: MMBench-Chinese](https://arxiv.org/abs/2307.06281)  |                                                              |
| [MM-Vet](https://arxiv.org/abs/2308.02490)[OKVQA](https://allenai.org/project/a-okvqa/home) | Evaluating Large Multimodal Models for Integrated Capabilities.knowledge-based visual question answering benchmark. |
| [POPE](https://github.com/RUCAIBox/POPE)                     | Evaluating Object Hallucination in Large Vision-Language Models. |
| [QBench](https://arxiv.org/abs/2309.14181)                   | A Benchmark for General-Purpose Foundation Models on Low-level Vision. |
| [SEED-I:  SEED-Bench (Image)](https://arxiv.org/abs/2307.16125) | Benchmarking Multimodal LLMs with Generative Comprehension.  |
| [SQA-I: ScienceQA-IMG](https://scienceqa.github.io/)         | Multimodal Reasoning via Thought Chains for Science Question Answering. |
| [VQA-v2](https://arxiv.org/abs/1612.00837)                   | Elevating the Role of Image Understanding in Visual Question Answering. |
| [VizWiz](https://arxiv.org/abs/1802.08218)                   | Answering Visual Questions from Blind People.                |
| [VQA-T: TextVQA](https://arxiv.org/abs/1904.08920)           | complementary to VQA 2.0.                                    |
| [VSR](https://arxiv.org/abs/2205.00363)                      | Visual Spatial Reasoning.                                    |





## Challenging Benchmark

| Name                                                         | Features                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [BenchLMM](https://github.com/AIFEG/BenchLMM)                | Cross-style Visual Capability of Large Multimodal Models.    |
| [CMMU](https://cmmmu-benchmark.github.io/)                   | A Chinese Massive Multi-discipline Multimodal Understanding Benchmark. |
| [GOAT-Bench](https://goatlmm.github.io/)                     |                                                              |
| [LLaVA-Bench](https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings) | Multimodal instruction-following benchmark.                  |
| [MMMU](https://mmmu-benchmark.github.io/)                    | Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. |
| [MathVista](https://mathvista.github.io/)                    | Evaluating Math Reasoning in Visual Contexts.                |

