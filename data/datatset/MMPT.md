## Multimodal Pre-training Datasets

***X  Open represents whether the dataset is open source,#.X represents the quantity of X, #.T represents the quantity of Text,and #.X-T represents the quantity of X-Text pairs, where X can be Image, Video, or Audio***

|                         Dataset Name                         |  X Open  | X Modality  |           #.X            |  #.T  |       #.X-T       |
| :----------------------------------------------------------: | :------: | :---------: | :----------------------: | :---: | :---------------: |
|          [ALIGN](https://arxiv.org/abs/2102.05918)           |          |    Image    |           1.8B           | 1.8B  |       1.8B        |
|        [AISHELL-1](https://arxiv.org/abs/2305.04160)         |          |    Audio    |            -             |   -   |       128K        |
|        [AISHELL-2](https://arxiv.org/abs/2305.04160)         |          |    Audio    |            -             |   -   |        1M         |
|  [AI Challenger Captions](https://arxiv.org/abs/1711.06475)  |          |    Image    |           300K           | 1.5M  |       1.5M        |
|     [A-OKVQA](https://allenai.org/project/a-okvqa/home)      | &#x2714; |    Image    |          23.7K           | 24.9K |       24.9K       |
| [CC3M](https://github.com/google-research-datasets/conceptual-captions) | &#x2714; |    Image    |           3.3M           | 3.3M  |       3.3M        |
| [CC12M](https://github.com/google-research-datasets/conceptual-12m) | &#x2714; |    Image    |          12.4M           | 12.4M |       12.4M       |
|      [COYO](https://github.com/kakaobrain/coyo-dataset)      | &#x2714; |    Image    |           747M           | 747M  |       747M        |
|       [COCO Caption](https://arxiv.org/abs/1504.00325)       |          |    Image    |           164K           |  1M   |        1M         |
|            [CC595k](https://llava-vl.github.io/)             | &#x2714; |    Image    |           595K           | 595K  |       595K        |
|              [DocVQA](https://www.docvqa.org/)               | &#x2714; |    Image    |           12K            |  50K  |        50K        |
|             [DataComp](https://www.datacomp.ai/)             | &#x2714; |    Image    |           1.4B           | 1.4B  |       1.4B        |
|           [DVQA](https://arxiv.org/abs/1801.08163)           |          |    Image    |           300K           | 3.5M  |       3.5M        |
|      [Episodic WebLI](https://arxiv.org/abs/2305.18565)      |          |    Image    |           400M           | 400M  |       400M        |
|   [Flickr30k](http://nlp.cs.illinois.edu/Denotation.html)    |          |    Image    |           31K            |  31K  |        31K        |
|           [GQA](https://arxiv.org/abs/1902.00751)            |          |    Image    |           113K           |  22M  |        22M        |
|           [LTIP](https://arxiv.org/abs/2204.14198)           |          |    Image    |           312M           | 312M  |       312M        |
|         [LAION-5B](https://laion.ai/blog/laion-5b/)          | &#x2714; |    Image    |           5.9B           | 5.9B  |       5.9B        |
|        [LAION-400M](https://arxiv.org/abs/2111.02114)        |          |    Image    |           400M           | 400M  |       400M        |
|         [LAION-en](https://laion.ai/blog/laion-5b/)          | &#x2714; |    Image    |           2.3B           | 2.3B  |       2.3B        |
|         [LAION-zh](https://laion.ai/blog/laion-5b/)          | &#x2714; |    Image    |           142M           | 142M  |       142M        |
|       [LAION-COCO](https://laion.ai/blog/laion-coco/)        | &#x2714; |    Image    |           600M           | 600M  |       600M        |
|          [MS-COCO](https://arxiv.org/abs/1405.0312)          |          |    Image    |           124K           | 620K  |       620K        |
|     [M3W(Interleaved)](https://arxiv.org/abs/2204.14198)     |          |    Image    |           185M           | 182G  | 43.3M(Instances)  |
|     [MMC4(Interleaved)](https://github.com/allenai/mmc4)     | &#x2714; |    Image    |           571M           |  43B  | 101.2M(Instances) |
| [MSRVTT](https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/) | &#x2714; |    Video    |           10K            | 200K  |       200K        |
|   [Obelics(Interleaved)](https://arxiv.org/abs/2306.16527)   |          |    Image    |           353M           | 115M  |  141M(Instances)  |
|            [OCR-VQA](https://ocr-vqa.github.io/)             | &#x2714; |    Image    |           207K           |  1M   |        1M         |
|             [OK-VQA](https://okvqa.allenai.org/)             | &#x2714; |    Image    |           14K            |  14K  |        14K        |
|         [RefCOCO](https://arxiv.org/abs/1608.00272)          |          |    Image    |           20K            | 142K  |       142K        |
|         [RefCOCO+](https://arxiv.org/abs/1608.00272)         |          |    Image    |           20K            | 142K  |       142K        |
|          [ST-VQA](https://arxiv.org/abs/2112.12494)          |          |    Image    |           23K            |  32K  |        32K        |
|  [SBU](https://dl.acm.org/doi/abs/10.5555/2986459.2986587)   |          |    Image    |            1M            |  1M   |        1M         |
|        [Text Captions](https://textvqa.org/textcaps/)        | &#x2714; |    Image    |           28K            | 145K  |       145K        |
|         [TextVQA](https://arxiv.org/abs/1904.08920)          |          |    Image    |          28.4K           | 45.3K |       45.3K       |
|      [Visual Genome](https://arxiv.org/abs/1602.07332)       |          |    Image    |           108K           | 4.5M  |       4.5M        |
|        [Visual-7W](https://arxiv.org/abs/1511.03416)         |          |    Image    |          47.3K           | 328K  |       328K        |
|           [VGQA](https://arxiv.org/abs/1602.07332)           |          |    Image    |           108K           | 1.7M  |       1.7M        |
|               [VQA-v2](https://visualqa.org/)                | &#x2714; |    Image    |           265K           | 1.4M  |       1.4M        |
|           [VTP](https://arxiv.org/abs/2204.14198)            |          |    Video    |           27M            |  27M  |        27M        |
|        [VSDial-CN](https://arxiv.org/abs/2305.19972)         |          | Image,Audio | 120K(Image), 1.2M(Audio) | 120K  |       1.2M        |
|  [Wukong](https://wukong-dataset.github.io/wukong-dataset/)  | &#x2714; |    Image    |           101M           | 101M  |       101M        |
|          [WebLI](https://arxiv.org/abs/2209.06794)           |          |    Image    |           10B            |  12B  |        12B        |
|       [WaveCaps](https://github.com/XinhaoMei/WavCaps)       | &#x2714; |    Audio    |           403K           | 403K  |       403K        |
|          [WebVid](https://arxiv.org/abs/2104.00650)          |          |    Video    |           10M            |  10M  |        10M        |
