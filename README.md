# Multi-Modal Large Model

[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT) [![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE)

Papers, codes, datasets, applications, tutorials of Multi-Modal Large Model.

**Widely used by top conferences and journals:**

- Conferences: [[NeurlPS](https://nips.cc/)] [[ICLR](https://iclr.cc/)] [[PMLR](https://proceedings.mlr.press/)] [[ICML](https://icml.cc/)] [[WACV](https://wacv2024.thecvf.com/)] [[EMNLP](https://aclanthology.org/venues/emnlp/)]
- Journals: [[TACL](https://transacl.org/index.php/tacl)]

**Related Codes:**

- LLM deployment tools: [[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)] [[LMDeploy](https://github.com/InternLM/lmdeploy)] [[llama.cpp](https://github.com/ggerganov/llama.cpp)] [[vllm](https://github.com/vllm-project/vllm)]



## 0.Survey(综述)

| Papers                                                       | Recommendation Index                 |
| ------------------------------------------------------------ | ------------------------------------ |
| [2023] [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549) |                                      |
| [2023] [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2311.13165) |                                      |
| [2023] [Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey](https://arxiv.org/abs/2312.16602) | :star::star::star::star::star:       |
| [2023] [A Survey on Multimodal Large Language Models for Autonomous Driving](https://arxiv.org/abs/2311.12320) |                                      |
| [2023] [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549) |                                      |
| [2024] [How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model](https://arxiv.org/abs/2311.07594) |                                      |
| [2024] [The (R)Evolution of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2402.12451) |                                      |
| [2024] [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601) | :star::star::star::star::star::star: |



## 1.Multi-Modal Areas and Papers(研究领域与论文)

- Mitgating Hallucination
- Embodied Intelligence
- Continual Learning
- [Multi-Modal LLM for 3D Scene](https://github.com/Evan-wyl/Multimodal-Large-Model/blob/master/papers/mmllm-3d.md)
- [Mobile/Lightweight Deployment](https://github.com/Evan-wyl/Multimodal-Large-Model/blob/master/papers/deploy.md)
- [Large Language Model Compressing](https://github.com/Evan-wyl/Multimodal-Large-Model/blob/master/papers/llm-compressing.md)



## 2.Code(代码)

Please see [HERE](https://github.com/Evan-wyl/Robot-Learning/tree/master/mm-lm/code) for the popular robot learning **code** results.

[这里](https://github.com/Evan-wyl/Robot-Learning/tree/master/mm-lm/code)整理了相关代码。



## 3.Datasets and Benchmarks(数据集与评测结果)

Please see [HERE](https://github.com/Evan-wyl/Robot-Learning/tree/master/mm-lm/data) for the popular multimodal large model **datasets and benchmark** results.

[这里](https://github.com/Evan-wyl/Robot-Learning/tree/master/mm-lm/data)整理了常用的公开数据集和一些已发表的文章在这些数据集上的实验结果。



## 4.Related Resource(相关资源)

- [mm-llm](https://mm-llms.github.io/)
- [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)



## Contributing (欢迎参与贡献)

If you are interested in contributing, please refer to [HERE](https://github.com/Evan-wyl/Multimodal-Large-Model/blob/master/CONTRIBUTING.md) for instructions in contribution.

------

### Copyright notice

> ***[Notes]This Github repo can be used by following the corresponding licenses. I want to emphasis that it may contain some PDFs or thesis, which were downloaded by me and can only be used for academic purposes. The copyrights of these materials are owned by corresponding publishers or organizations. All this are for better adademic research. If any of the authors or publishers have concerns, please contact me to delete or replace them.***