## Large Model Compressing

### Quantization

[2022] [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2211.10438)

[2022] [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)

[2023] [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)

[2023] [Norm Tweaking: High-performance Low-bit Quantization of Large Language Models](https://arxiv.org/abs/2309.02784)

[2023] [SqueezeLLM: Dense-and-Sparse Quantization](https://arxiv.org/abs/2306.07629)



### Pruning

[2023] [A Simple and Effective Pruning Approach for Large Language Models](https://arxiv.org/abs/2306.11695)

[2023] [LLM-Pruner: On the Structural Pruning of Large Language Models](https://arxiv.org/abs/2305.11627)

[2023] [SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot](https://arxiv.org/abs/2301.00774)



### Knowledge Distilling

[2023] [Lifting the Curse of Capacity Gap in Distilling Language Models](https://arxiv.org/abs/2305.12129)



### Low-Rank Decomposition

[2023] [ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation](https://arxiv.org/abs/2303.08302)